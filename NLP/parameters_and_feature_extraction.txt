Good parameters for the classifiers
All data trained using parameter_tuning.py.

NuSVC classifier data (with movie_reviews) [3/7/16]:

(n = 30)
+------+----------+
|  Nu  | Accuracy |
+------+----------+
|  0.4 |     71.0 |
|  0.5 |     73.8 |
|  0.6 |     72.3 |
|  0.7 |     72.9 |
| 0.75 |     72.8 |
|  0.8 |     73.0 | <==
+------+----------+

Best nu value is 0.8

Degree is inconsequential, so degree will be 3. As well, changing decision_function_shape to "ovr" is recommended so
it will be done.

RandomForestClassifier (with movie_reviews) [4/7/16]:

(n = 30)
+---------------------------+----------+
| n_estimators (# of trees) | Accuracy |
+---------------------------+----------+
|                         6 |     64.0 |
|                         8 |     64.1 |
|                        10 |     65.9 |
|                        12 |     66.8 |
|                        14 |     67.8 |
|                        16 |     69.1 |
|                        20 |     71.6 |
|                        25 |     75.1 | <==
+---------------------------+----------+

n_estimators = 25 is the point at which the growth in accuracy slows down significantly but the running time increases
significantly, so n_estimator = 25 is the best value.

(n = 30)
+------------------+----------+
| min_samples_leaf | Accuracy |
+------------------+----------+
|                1 |     74.4 |
|                2 |     75.0 |
|                3 |     75.1 |
|                4 |     74.9 |
|                5 |     74.5 |
|                6 |     76.6 | <==
|                7 |     75.9 |
|                8 |     74.4 |
|                9 |     75.7 |
|               10 |     73.9 |
|               11 |     75.2 |
|               12 |     74.5 |
|               13 |     74.0 |
|               14 |     74.3 |
|               15 |     73.5 |
|               16 |     73.7 |
|               17 |     72.8 |
|               18 |     74.3 |
|               19 |     74.1 |
|               20 |     71.1 |
|               25 |     69.9 |
|               30 |     67.7 |
|               35 |     69.2 |
|               40 |     67.7 |
|               45 |     68.0 |
|               50 |     65.7 |
|               55 |     63.1 |
|               60 |     64.6 |
|               65 |     62.5 |
|               70 |     62.8 |
|               75 |     62.2 |
|               80 |     64.3 |
|               85 |     60.5 |
|               90 |     61.8 |
|               95 |     59.4 |
|              100 |     57.9 |
+------------------+----------+

The best min_samples_leaf is 6.
A graph of this data can be found at min_samples_leaf_to_accuracy.png.

The average accuracy of the OpinionLexiconClassifier in opinion_lexicon_classifier.py (n = 100) is 62.2.

The average accuracy of the LinearSVC algorithm (n = 60) is 78.3.

(n = 30)
+-----+----------+
|  C  | Accuracy |
+-----+----------+
| 0.2 |     76.9 |
| 0.4 |     77.8 |
| 0.6 |     78.9 |
| 0.8 |     78.4 |
| 1.0 |     78.3 | <==
| 1.2 |     76.8 |
| 1.4 |     78.6 |
| 1.6 |     77.1 |
| 1.8 |     77.1 |
| 2.0 |     78.0 |
| 2.2 |     76.9 |
| 2.4 |     77.6 |
| 2.6 |     77.7 |
| 2.8 |     77.6 |
| 3.0 |     78.1 |
| 3.2 |     78.6 |
| 3.4 |     77.8 |
| 3.6 |     78.5 |
| 3.8 |     75.7 |
| 4.0 |     76.3 |
| 4.2 |     76.8 |
| 4.4 |     77.0 |
| 4.6 |     77.3 |
| 4.8 |     77.7 |
| 5.0 |     78.4 |
| 5.2 |     78.3 |
| 5.4 |     77.4 |
| 5.6 |     77.6 |
| 5.8 |     78.0 |
| 6.0 |     77.9 |
| 6.2 |     78.6 |
| 6.4 |     78.9 |
| 6.6 |     77.8 |
| 6.8 |     76.9 |
| 7.0 |     77.7 |
| 7.2 |     79.2 |
| 7.4 |     77.8 |
| 7.6 |     78.0 |
| 7.8 |     78.0 |
| 8.0 |     76.9 |
| 8.2 |     77.5 |
| 8.4 |     77.6 |
| 8.6 |     77.2 |
| 8.8 |     78.2 |
| 9.0 |     79.4 |
| 9.2 |     77.2 |
| 9.4 |     77.3 |
| 9.6 |     77.7 |
| 9.8 |     77.9 |
+-----+----------+

Although C values at 9.0 and 7.2 do provide accuracy above 79.0%, this is hardly statistically significant. It seems
that by C = 0.6 the C value has hit a critical mass and after this point raising the C value provides no increase in
accuracy. For this reason, C = 1.0.

The default accuracy of the scikit learn Support Vector Classifier (SVC) is ~50%. However, this can be greatly increased
by upping the C value.

(n = 30)
+-----+----------+-----------------------------+
|  C  | Accuracy | Average Training Time (sec) |
+-----+----------+-----------------------------+
|   5 |     75.3 |                        23.2 |
|  10 |     78.3 |                        22.6 |
|  15 |     80.4 |                        22.3 | <==
|  20 |     79.1 |                        22.1 |
|  25 |     80.0 |                        22.0 |
|  30 |     80.6 |                        21.9 |
|  35 |     79.7 |                        21.8 |
|  40 |     80.2 |                        21.7 |
|  45 |     80.7 |                        21.8 |
|  50 |     79.9 |                        21.7 |
|  55 |     80.7 |                        21.5 |
|  60 |     80.7 |                        21.6 |
|  65 |     79.5 |                        21.5 |
|  70 |     80.2 |                        21.4 |
|  75 |     80.1 |                        21.4 |
|  80 |     81.2 |                        21.4 |
|  85 |     81.0 |                        21.4 |
|  90 |     80.6 |                        21.4 |
|  95 |     80.0 |                        21.3 |
| 100 |     81.2 |                        21.3 |
+-----+----------+-----------------------------+

C value of 15 seems to provide sufficient accuracy. As well, changing decision_function_shape to "ovr" is recommended so
it will be done.

(n = 30)
+--------+----------+-----------------------------+
| Degree | Accuracy | Average Training Time (sec) |
+--------+----------+-----------------------------+
|      1 |     47.8 |                        22.9 |
|      2 |     50.3 |                        22.9 |
|      3 |     47.9 |                        22.9 |
|      4 |     48.3 |                        22.8 |
|      5 |     46.4 |                        22.9 |
|      6 |     51.8 |                        22.9 |
|      7 |     50.2 |                        22.9 |
|      8 |     46.3 |                        22.8 |
|      9 |     46.3 |                        22.9 |
|     10 |     46.5 |                        22.9 |
+--------+----------+-----------------------------+

The degree remains inconsequential, so it will be kept at the default of 3.

The scikit learn Stochastic Gradient Descent Classifier (SGDC) provides a default accuracy of 77.4%.

(n = 30)
+---------------------------+----------+---------------------+
|           Loss            | Accuracy | Training Time (sec) |
+---------------------------+----------+---------------------+
| hinge                     |     78.8 |                20.2 | <==
| log                       |     77.1 |                18.1 |
| modifier_huber            |     78.3 |                18.0 |
| squared_hinge             |     77.0 |                18.0 |
| perceptron                |     76.8 |                18.0 |
| squared_loss              |     48.3 |                18.0 |
| huber                     |     53.7 |                18.0 |
| epsilon_insensitive       |     50.4 |                18.0 |
| squared_epsilon_intensive |     49.7 |                18.1 |
+---------------------------+----------+---------------------+

The default of hinge provides the best results, so it will remain.

The scikit learn Logistic Regression algorithm provides a default accuracy of 77.9%.

(n = 30)
+----+----------+---------------------+
| C  | Accuracy | Training Time (sec) |
+----+----------+---------------------+
|  2 |     79.4 |                18.9 | <==
|  4 |     79.6 |                22.5 |
|  6 |     80.2 |                21.9 |
|  8 |     79.5 |                20.3 |
| 10 |     80.0 |                18.2 |
| 12 |     80.0 |                18.6 |
| 14 |     79.4 |                20.3 |
| 16 |     80.1 |                17.6 |
| 18 |     79.2 |                17.6 |
| 20 |     80.5 |                17.5 |
| 22 |     78.5 |                17.5 |
| 24 |     80.0 |                17.6 |
| 26 |     79.6 |                17.5 |
| 28 |     79.9 |                17.6 |
| 30 |     78.7 |                17.5 |
| 32 |     79.2 |                17.5 |
| 34 |     79.4 |                17.5 |
| 36 |     80.1 |                17.4 |
| 38 |     78.4 |                17.6 |
| 40 |     80.0 |                17.5 |
| 42 |     79.7 |                17.6 |
| 44 |     78.7 |                17.6 |
| 46 |     79.6 |                17.4 |
| 48 |     79.4 |                17.4 |
+----+----------+---------------------+

Since C seems inconsequential, it will be set to 2.

The Bernoulli Naïve Bayes provides a default accuracy of 77.5% with training time of 22.1 seconds (n = 60). There are no
parameters to tune.

The Multinomial Naïve Bayes provides a default accuracy of 79.0% with training time of 21.0 seconds (n = 60). There are no
parameters to tune.

The AdaBoostClassifier provides a default accuracy of 73.6% with a training time of 18.8 seconds (n = 60).

(n = 30)
+--------------+----------+---------------------+
| n_estimators | Accuracy | Training Time (sec) |
+--------------+----------+---------------------+
|            5 |     62.6 |                18.2 |
|           10 |     67.5 |                18.3 |
|           15 |     67.5 |                18.4 |
|           20 |     70.2 |                18.3 |
|           25 |     69.2 |                18.3 |
|           30 |     71.5 |                18.4 |
|           35 |     73.2 |                18.6 |
|           40 |     72.9 |                18.5 |
|           45 |     73.4 |                18.6 |
|           50 |     74.9 |                18.8 |
|           55 |     74.9 |                19.0 |
|           60 |     75.7 |                19.1 | <==
|           65 |     75.6 |                19.3 |
|           70 |     74.1 |                19.1 |
|           75 |     74.3 |                19.2 |
|           80 |     76.1 |                19.3 |
|           85 |     75.9 |                19.6 |
|           90 |     75.9 |                19.4 |
|           95 |     75.6 |                19.8 |
|          100 |     76.3 |                19.6 |
+--------------+----------+---------------------+

The default of 50 provides adequate accuracy, so it shall remain.

DON'T USE:
NLTK MAXENTCLASSIFIER
NLTK DECISIONTREECLASSIFIER
NLTK RANDOMFORESTCLASSIFIER
SCIKIT LEARN GAUSSIANNB

------------------------------------------------------------------------------------------------------------------------
Feature Extraction
All data gathered from parameter_tuning.py

Using stop word filtering yielded the following results for the NuSVC classifier with nu = 0.75 (n = 10):
66.97% Accuracy (with an old data set)

Stemming yielded a loss of ~5-10% accuracy (on average)

(n = 30)
+----------------------------+----------+
| # of Top Features Included | Accuracy |
+----------------------------+----------+
|                       2000 |     67.9 |
|                       3000 |     71.5 |
|                       4000 |     70.6 |
|                       5000 |     75.2 |
|                       6000 |     75.4 |
|                       7000 |     76.9 |
|                       8000 |     77.2 |
|                       9000 |     79.3 | <==
|                      10000 |     80.1 |
|                      11000 |     81.1 |
|                      12000 |     81.1 |
|                      13000 |     80.1 |
|                      14000 |     81.7 |
|                      20000 |     82.3 |
+----------------------------+----------+

9000 seems to be the point at which the increase in accuracy slows down the most. For now, 9000 will be used.

Using only adverbs (as apposed to adverbs and verbs) in featureset provides an increase in accuracy.

Surprisingly, bigrams provided no advantage in accuracy and in general only made training times slower.

+---------------+-----------------+----------+
| Top all_words | Top all_bigrams | Accuracy |
+---------------+-----------------+----------+
|             0 |            9000 |     50.1 |
|          1000 |            8000 |     64.9 |
|          2000 |            7000 |     68.4 |
|          3000 |            6000 |     70.6 |
|          4000 |            5000 |     71.0 |
|          5000 |            4000 |     71.9 |
|          6000 |            3000 |     73.3 |
|          7000 |            2000 |     74.5 |
|          8000 |            1000 |     76.1 |
|          9000 |               0 |     76.1 | <==
|          9000 |             500 |     74.8 |
|          9000 |            1000 |     75.4 |
|          9000 |            1500 |     74.9 |
|          9000 |            2000 |     75.8 |
|          9000 |            2500 |     74.3 |
|          9000 |            3000 |     75.9 |
|          9000 |            3500 |     74.8 |
|          9000 |            4000 |     75.2 |
|          9000 |            4500 |     74.9 |
|          9000 |            5000 |     74.9 |
|          9000 |            5500 |     75.4 |
|          9000 |            6000 |     75.5 |
|          9000 |            6500 |     75.1 |
|          9000 |            7000 |     75.8 |
|          9000 |            7500 |     75.6 |
|          9000 |            8000 |     74.3 |
|          9000 |            8500 |     74.1 |
|          9000 |            9000 |     75.4 |
+---------------+-----------------+----------+

The original configuration of the top 9000 of all_words and no bigrams yields the best results. Although trigrams and
higher ngrams (with n as high as 5) are recommended, it seems that even bigrams simply make way to much noise, so other
ngrams won't be considered or tested.

------------------------------------------------------------------------------------------------------------------------

Old data:

NuSVC classifier data (with short_reviews) [2/7/16]:

(n = 10):
+------+----------+
|  Nu  | Accuracy |
+------+----------+
|  0.4 |    60.97 |
| 0.45 |    63.55 |
|  0.5 |    63.65 |
| 0.55 |    64.76 |
|  0.6 |    66.00 |
| 0.65 |    65.36 |
|  0.7 |    67.12 |
| 0.75 |    67.23 | <==
|  0.8 |    66.74 |
| 0.85 |    67.17 |
+------+----------+

Best nu value is 0.75.

(n = 10)
+--------+----------+
| Degree | Accuracy |
+--------+----------+
|      2 |    67.02 |
|      3 |    66.40 |
|      4 |    66.94 |
+--------+----------+

Degree is inconsequential, so degree is 3.
